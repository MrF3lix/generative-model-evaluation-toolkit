{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f08e2dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from omegaconf import OmegaConf\n",
    "from sklearn.metrics import f1_score, multilabel_confusion_matrix, matthews_corrcoef, confusion_matrix\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "\n",
    "\n",
    "from cgeval.method import ClassifyAndCount, StandardClassification, BCC\n",
    "from cgeval.rating import Ratings, Label, Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19d1b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = {\n",
    "    'llama 2': '../../out/pipeline/2025-05-15_sentiment_analysis_llama2',\n",
    "    'llama 3.3': '../../out/pipeline/2025-05-15_sentiment_analysis_llama3-3',\n",
    "    'mistral': '../../out/pipeline/2025-05-15_sentiment_analysis_mistral',\n",
    "}\n",
    "\n",
    "REPORT_PATH = '../../out/pipeline/2025-04-30_15-26_sentiment_analysis_llama2'\n",
    "CONFIG = '../../config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d13d5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_name_to_id(name: str, labels) -> int:\n",
    "    return next((l['id'] for l in labels if l['name'] == name), None)\n",
    "\n",
    "def label_match_to_id(match: str, matching_label: str) -> int:\n",
    "    return int(match == matching_label) if match is not None else match\n",
    "\n",
    "def extract_sentiment(input):\n",
    "    m = re.search('The story should have a (.+?) sentiment', input)\n",
    "    if m:\n",
    "        found = m.group(1)\n",
    "        return found\n",
    "\n",
    "    return input\n",
    "\n",
    "def load_ratings(cfg, classifier, report_path):\n",
    "    with open(f\"{report_path}/evaluate/dataset_{classifier.id}.json\", 'r') as f:\n",
    "        ratings_data = json.load(f)\n",
    "\n",
    "    observations = list(map(lambda i: Observation(\n",
    "        id=i['id'],\n",
    "        output=i['output'],\n",
    "        input=label_name_to_id(i['input'], classifier.labels),\n",
    "        oracle=label_name_to_id(i['oracle'], classifier.labels),\n",
    "        metric=label_name_to_id(i['metric'], classifier.labels)\n",
    "    ), ratings_data))\n",
    "\n",
    "    labels = list(map(lambda l: Label(**l), classifier.labels))\n",
    "\n",
    "    return Ratings(labels=labels, observations=observations)\n",
    "\n",
    "def load_binary_ratings(classifier, report_path):\n",
    "    df = pd.read_json(f\"{report_path}/evaluate/dataset_{classifier.id}.json\", orient='records')\n",
    "\n",
    "    df['condition'] = df['input'].apply(extract_sentiment)\n",
    "    df['oracle'] = df.apply(lambda r: r['oracle'] == r['condition'] if(pd.notnull(r['oracle'])) else r['oracle'], axis=1)\n",
    "    df['metric'] = df.apply(lambda r: r['metric'] == r['condition'], axis=1)\n",
    "\n",
    "    observations = df.to_dict(orient='records')\n",
    "    observations = list(map(lambda i: Observation(\n",
    "        id=i['id'],\n",
    "        output=i['output'],\n",
    "        input=1,\n",
    "        oracle=label_match_to_id(i['oracle'], True),\n",
    "        metric=label_match_to_id(i['metric'], True)\n",
    "    ), observations))\n",
    "\n",
    "    labels = list(map(lambda l: Label(**l), [{'id': 0, 'name': 'match'},{'id': 1, 'name': 'no_match'}]))\n",
    "\n",
    "    return Ratings(labels=labels, observations=observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a79101bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mixture_matrix(rating):\n",
    "    items = [o for o in rating.observations if o.oracle is not None]\n",
    "\n",
    "    oracle_ratings = [o.oracle for o in items]\n",
    "    metric_ratings = [o.metric for o in items]\n",
    "\n",
    "\n",
    "    return multilabel_confusion_matrix(oracle_ratings, metric_ratings, labels=rating.get_label_ids())\n",
    "\n",
    "def compute_single_mixture_matrix(rating):\n",
    "    items = [o for o in rating.observations if o.oracle is not None]\n",
    "\n",
    "    oracle_ratings = [o.oracle for o in items]\n",
    "    metric_ratings = [o.metric for o in items]\n",
    "\n",
    "\n",
    "    return confusion_matrix(oracle_ratings, metric_ratings, labels=rating.get_label_ids())\n",
    "\n",
    "def compute_f1_score(rating):\n",
    "    items = [o for o in rating.observations if o.oracle is not None]\n",
    "\n",
    "    oracle_ratings = [o.oracle for o in items]\n",
    "    metric_ratings = [o.metric for o in items]\n",
    "\n",
    "\n",
    "    return f1_score(oracle_ratings, metric_ratings, labels=rating.get_label_ids(), average='macro')\n",
    "\n",
    "\n",
    "def compute_spearman(rating):\n",
    "    items = [o for o in rating.observations if o.oracle is not None]\n",
    "\n",
    "    oracle_ratings = [o.oracle for o in items]\n",
    "    metric_ratings = [o.metric for o in items]\n",
    "\n",
    "\n",
    "    return spearmanr(oracle_ratings, metric_ratings)\n",
    "\n",
    "\n",
    "def compute_pointbiserialr(rating):\n",
    "    items = [o for o in rating.observations if o.oracle is not None]\n",
    "\n",
    "    oracle_ratings = [o.oracle for o in items]\n",
    "    metric_ratings = [o.metric for o in items]\n",
    "\n",
    "\n",
    "    return pointbiserialr(oracle_ratings, metric_ratings)\n",
    "\n",
    "def compute_matthews_corrcoef(rating):\n",
    "    items = [o for o in rating.observations if o.oracle is not None]\n",
    "\n",
    "    oracle_ratings = [o.oracle for o in items]\n",
    "    metric_ratings = [o.metric for o in items]\n",
    "\n",
    "    return matthews_corrcoef(oracle_ratings, metric_ratings)\n",
    "\n",
    "\n",
    "def pretty_print_latex(latex_str):\n",
    "    lines = latex_str.replace(r\" \\\\ \", r\" \\\\\" + \"\\n\").splitlines()\n",
    "    formatted_lines = []\n",
    "    indent_level = 0\n",
    "    for line in lines:\n",
    "        if r\"\\begin\" in line:\n",
    "            formatted_lines.append(line)\n",
    "            indent_level += 1\n",
    "        elif r\"\\end\" in line:\n",
    "            indent_level -= 1\n",
    "            formatted_lines.append(line)\n",
    "        else:\n",
    "            formatted_lines.append(\"    \" * indent_level + line)\n",
    "    return \"\\n\".join(formatted_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "26524ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing mistral: 100%|██████████| 9/9 [5:33:04<00:00, 2220.48s/it]\n",
      "Processing mistral: 100%|██████████| 9/9 [00:07<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load(CONFIG)\n",
    "\n",
    "tpr = np.zeros((3,3))\n",
    "fpr = np.zeros((3,3))\n",
    "f1 = np.zeros(3)\n",
    "p = np.zeros(3)\n",
    "\n",
    "pbar = tqdm(total=len(MODEL)*len(cfg.classifier))\n",
    "\n",
    "for id, base_path in MODEL.items():\n",
    "    pbar.set_description(f'Processing {id}')\n",
    "    for i, cls in enumerate(cfg.classifier):\n",
    "        pbar.update(1)\n",
    "        r = load_ratings(cfg, cls, base_path)\n",
    "        cm = compute_mixture_matrix(r)\n",
    "\n",
    "        f1[i] += compute_f1_score(r)\n",
    "        p[i] += compute_spearman(r).statistic\n",
    "\n",
    "        for idx in r.get_label_ids():\n",
    "            (tn, fp, fn, tp) = cm[idx].ravel()\n",
    "            label_name = cls.labels[idx]['name']\n",
    "            tpr[idx][i] += tp / (tp + fn)\n",
    "            fpr[idx][i] += fp / (fp + tn)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "tpr = tpr / len(MODEL)\n",
    "fpr = fpr / len(MODEL)\n",
    "f1 = f1 / len(MODEL)\n",
    "p = p / len(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fa8c032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing mistral: 100%|██████████| 9/9 [00:01<00:00,  7.41it/s]  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.327, 0.696, 0.844]), array([0.913, 0.   , 0.052]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbar = tqdm(total=len(MODEL)*len(cfg.classifier))\n",
    "\n",
    "tpr_match = np.zeros(3)\n",
    "fpr_match = np.zeros(3)\n",
    "for id, base_path in MODEL.items():\n",
    "    pbar.set_description(f'Processing {id}')\n",
    "    for i, cls in enumerate(cfg.classifier):\n",
    "        pbar.update(1)\n",
    "        r = load_binary_ratings(cls, base_path)\n",
    "        cm = compute_single_mixture_matrix(r)\n",
    "        (tn, fp, fn, tp) = cm.ravel()\n",
    "\n",
    "        tpr_match[i] += tp / (tp + fn)\n",
    "        fpr_match[i] += fp / (fp + tn)\n",
    "\n",
    "tpr_match = tpr_match / len(MODEL)\n",
    "fpr_match = fpr_match / len(MODEL)\n",
    "tpr_match = np.round(tpr_match, 3)\n",
    "fpr_match = np.round(fpr_match, 3)\n",
    "\n",
    "tpr_match, fpr_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6eb1e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = np.round(tpr, 3)\n",
    "fpr = np.round(fpr, 3)\n",
    "f1 = np.round(f1, 3)\n",
    "p = np.round(p, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b2fc982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th></th>\n",
       "            <th>FIB</th>\n",
       "            <th>DSS</th>\n",
       "            <th>LL3</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>Macro F1</td>\n",
       "            <td>0.275</td>\n",
       "            <td>0.521</td>\n",
       "            <td>0.671</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Spearman</td>\n",
       "            <td>0.263</td>\n",
       "            <td>0.598</td>\n",
       "            <td>0.914</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>TPR_positive</td>\n",
       "            <td>0.216</td>\n",
       "            <td>0.695</td>\n",
       "            <td>1.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>FPR_positive</td>\n",
       "            <td>0.074</td>\n",
       "            <td>0.127</td>\n",
       "            <td>0.195</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>TPR_neutral</td>\n",
       "            <td>0.886</td>\n",
       "            <td>0.045</td>\n",
       "            <td>0.089</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>FPR_neutral</td>\n",
       "            <td>0.784</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>TPR_negative</td>\n",
       "            <td>0.129</td>\n",
       "            <td>0.943</td>\n",
       "            <td>0.99</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>FPR_negative</td>\n",
       "            <td>0.016</td>\n",
       "            <td>0.388</td>\n",
       "            <td>0.063</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>TPR_match</td>\n",
       "            <td>0.327</td>\n",
       "            <td>0.696</td>\n",
       "            <td>0.844</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>FPR_match</td>\n",
       "            <td>0.913</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.052</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+--------------+-------+-------+-------+\n",
       "|              |  FIB  |  DSS  |  LL3  |\n",
       "+--------------+-------+-------+-------+\n",
       "|   Macro F1   | 0.275 | 0.521 | 0.671 |\n",
       "|   Spearman   | 0.263 | 0.598 | 0.914 |\n",
       "| TPR_positive | 0.216 | 0.695 |  1.0  |\n",
       "| FPR_positive | 0.074 | 0.127 | 0.195 |\n",
       "| TPR_neutral  | 0.886 | 0.045 | 0.089 |\n",
       "| FPR_neutral  | 0.784 |  0.0  |  0.0  |\n",
       "| TPR_negative | 0.129 | 0.943 |  0.99 |\n",
       "| FPR_negative | 0.016 | 0.388 | 0.063 |\n",
       "|  TPR_match   | 0.327 | 0.696 | 0.844 |\n",
       "|  FPR_match   | 0.913 |  0.0  | 0.052 |\n",
       "+--------------+-------+-------+-------+"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = PrettyTable()\n",
    "t.add_column('', ['Macro F1', 'Spearman', 'TPR_positive', 'FPR_positive', 'TPR_neutral', 'FPR_neutral', 'TPR_negative', 'FPR_negative'])\n",
    "t.add_column('FIB', [f1[0], p[0], tpr[0][0], fpr[0][0], tpr[1][0], fpr[1][0], tpr[2][0], fpr[2][0]])\n",
    "t.add_column('DSS', [f1[1], p[1], tpr[0][1], fpr[0][1], tpr[1][1], fpr[1][1], tpr[2][1], fpr[2][1]])\n",
    "t.add_column('LL3', [f1[2], p[2], tpr[0][2], fpr[0][2], tpr[1][2], fpr[1][2], tpr[2][2], fpr[2][2]])\n",
    "\n",
    "t.add_row(['TPR_match',tpr_match[0],tpr_match[1],tpr_match[2]])\n",
    "t.add_row(['FPR_match',fpr_match[0],fpr_match[1],fpr_match[2]])\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "371520b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{cccc}\n",
      "     & FIB & DSS & LL3 \\\\\n",
      "    Macro F1 & 0.275 & 0.521 & 0.671 \\\\\n",
      "    Spearman & 0.263 & 0.598 & 0.914 \\\\\n",
      "    TPR_positive & 0.216 & 0.695 & 1.0 \\\\\n",
      "    FPR_positive & 0.074 & 0.127 & 0.195 \\\\\n",
      "    TPR_neutral & 0.886 & 0.045 & 0.089 \\\\\n",
      "    FPR_neutral & 0.784 & 0.0 & 0.0 \\\\\n",
      "    TPR_negative & 0.129 & 0.943 & 0.99 \\\\\n",
      "    FPR_negative & 0.016 & 0.388 & 0.063 \\\\\n",
      "    TPR_match & 0.327 & 0.696 & 0.844 \\\\\n",
      "    FPR_match & 0.913 & 0.0 & 0.052 \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "l = pretty_print_latex(t.get_latex_string())\n",
    "\n",
    "print(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
